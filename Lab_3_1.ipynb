{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swLDp-GV7iQg"
      },
      "source": [
        "# Week 3: Basic Document Classification (Part 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLh_fS4P7iQn"
      },
      "source": [
        "## Overview\n",
        "In labs this week (and next), the focus will be on the application of sentiment analysis. You will be using a corpus of **movie reviews**.\n",
        "\n",
        "You will be exploring various techniques that can be used to classify the sentiment of the movie reviews as either positive or negative.\n",
        "\n",
        "You will be developing your own **Word List** and **Naïve Bayes** classifiers and then comparing them to the **NLTK Naïve Bayes** classifier.\n",
        "\n",
        "First, we will need to download the movie_review corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3W2AdikDqe5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422be883-9eca-444a-b629-80792761b9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0uVSM40qe5I"
      },
      "source": [
        "The movie_reviews corpus reader provides a number of useful methods:\n",
        "   * .categories()\n",
        "   * .fileids()\n",
        "   * .words()\n",
        "   \n",
        "First, we can use `.categories()` to check the set of labels with which the reviews have been labelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N3bRgahNqe5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbac7ae2-43d3-4eca-d081-a7281d833b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'pos']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "print(movie_reviews.categories())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INtoThcCqe5J"
      },
      "source": [
        "We can use `.fileids()` to get all of the file names associated with a particular category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LZ00H5mdqe5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409b6f68-1237-4915-ad15-0b84ea5c499c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of positive reviews is 1000\n",
            "The number of negative reviews is 1000\n"
          ]
        }
      ],
      "source": [
        "pos_review_ids=movie_reviews.fileids('pos')\n",
        "neg_review_ids=movie_reviews.fileids('neg')\n",
        "\n",
        "print(\"The number of positive reviews is {}\".format(len(pos_review_ids)))\n",
        "print(\"The number of negative reviews is {}\".format(len(neg_review_ids)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHxpQ6Sqe5L"
      },
      "source": [
        "We can use `.words()` to get back word-tokenised reviews.  The argument to `.words()` is the file id of an individual review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMvFYBPyqe5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a831e502-8a10-43e3-ef6b-4a7ae844f531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n"
          ]
        }
      ],
      "source": [
        "print(movie_reviews.words(pos_review_ids[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68LWdfszqe5M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "219e85a8-2275-44c3-ebcf-35a45f56c52b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.corpus.reader.util.StreamBackedCorpusView"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>nltk.corpus.reader.util.StreamBackedCorpusView</b><br/>def __init__(fileid, block_reader=None, startpos=0, encoding=&#x27;utf8&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/nltk/corpus/reader/util.py</a>A &#x27;view&#x27; of a corpus file, which acts like a sequence of tokens:\n",
              "it can be accessed by index, iterated over, etc.  However, the\n",
              "tokens are only constructed as-needed -- the entire corpus is\n",
              "never stored in memory at once.\n",
              "\n",
              "The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
              "a corpus fileid (specified as a string or as a ``PathPointer``);\n",
              "and a block reader.  A &quot;block reader&quot; is a function that reads\n",
              "zero or more tokens from a stream, and returns them as a list.  A\n",
              "very simple example of a block reader is:\n",
              "\n",
              "    &gt;&gt;&gt; def simple_block_reader(stream):\n",
              "    ...     return stream.readline().split()\n",
              "\n",
              "This simple block reader reads a single line at a time, and\n",
              "returns a single token (consisting of a string) for each\n",
              "whitespace-separated substring on the line.\n",
              "\n",
              "When deciding how to define the block reader for a given\n",
              "corpus, careful consideration should be given to the size of\n",
              "blocks handled by the block reader.  Smaller block sizes will\n",
              "increase the memory requirements of the corpus view&#x27;s internal\n",
              "data structures (by 2 integers per block).  On the other hand,\n",
              "larger block sizes may decrease performance for random access to\n",
              "the corpus.  (But note that larger block sizes will *not*\n",
              "decrease performance for iteration.)\n",
              "\n",
              "Internally, ``CorpusView`` maintains a partial mapping from token\n",
              "index to file position, with one entry per block.  When a token\n",
              "with a given index *i* is requested, the ``CorpusView`` constructs\n",
              "it as follows:\n",
              "\n",
              "  1. First, it searches the toknum/filepos mapping for the token\n",
              "     index closest to (but less than or equal to) *i*.\n",
              "\n",
              "  2. Then, starting at the file position corresponding to that\n",
              "     index, it reads one block at a time using the block reader\n",
              "     until it reaches the requested token.\n",
              "\n",
              "The toknum/filepos mapping is created lazily: it is initially\n",
              "empty, but every time a new block is read, the block&#x27;s\n",
              "initial token is added to the mapping.  (Thus, the toknum/filepos\n",
              "map has one entry per block.)\n",
              "\n",
              "In order to increase efficiency for random access patterns that\n",
              "have high degrees of locality, the corpus view may cache one or\n",
              "more blocks.\n",
              "\n",
              ":note: Each ``CorpusView`` object internally maintains an open file\n",
              "    object for its underlying corpus file.  This file should be\n",
              "    automatically closed when the ``CorpusView`` is garbage collected,\n",
              "    but if you wish to close it manually, use the ``close()``\n",
              "    method.  If you access a ``CorpusView``&#x27;s items after it has been\n",
              "    closed, the file object will be automatically re-opened.\n",
              "\n",
              ":warning: If the contents of the file are modified during the\n",
              "    lifetime of the ``CorpusView``, then the ``CorpusView``&#x27;s behavior\n",
              "    is undefined.\n",
              "\n",
              ":warning: If a unicode encoding is specified when constructing a\n",
              "    ``CorpusView``, then the block reader may only call\n",
              "    ``stream.seek()`` with offsets that have been returned by\n",
              "    ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
              "    relative offsets, or with offsets based on string lengths, may\n",
              "    lead to incorrect behavior.\n",
              "\n",
              ":ivar _block_reader: The function used to read\n",
              "    a single block from the underlying file stream.\n",
              ":ivar _toknum: A list containing the token index of each block\n",
              "    that has been processed.  In particular, ``_toknum[i]`` is the\n",
              "    token index of the first token in block ``i``.  Together\n",
              "    with ``_filepos``, this forms a partial mapping between token\n",
              "    indices and file positions.\n",
              ":ivar _filepos: A list containing the file position of each block\n",
              "    that has been processed.  In particular, ``_toknum[i]`` is the\n",
              "    file position of the first character in block ``i``.  Together\n",
              "    with ``_toknum``, this forms a partial mapping between token\n",
              "    indices and file positions.\n",
              ":ivar _stream: The stream used to access the underlying corpus file.\n",
              ":ivar _len: The total number of tokens in the corpus, if known;\n",
              "    or None, if the number of tokens is not yet known.\n",
              ":ivar _eofpos: The character position of the last character in the\n",
              "    file.  This is calculated when the corpus view is initialized,\n",
              "    and is used to decide when the end of file has been reached.\n",
              ":ivar _cache: A cache of the most recently read block.  It\n",
              "   is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
              "   start_toknum is the token index of the first token in the block;\n",
              "   end_toknum is the token index of the first token not in the\n",
              "   block; and tokens is a list of the tokens in the block.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 32);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "type(movie_reviews.words(pos_review_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkD_UEKqe5N"
      },
      "source": [
        "Note, the object returned by `movie_reviews.words()` looks a lot like a list (and behaves a lot like a list) - but it is actually a `StreamBackedCorpusView`.  This essentially means it is not necessarily all in memory  - it is retrieved from disk as needed.  If you want to see all of the words at once then you can convert it to a list using the `list()` constructor.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwmV4eh9qe5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b3383a-7cfb-4855-ca22-b9437eda992e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', 'there', \"'\", 's', 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'\", '80s', 'with', 'a', '12', '-', 'part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.', 'in', 'other', 'words', ',', 'don', \"'\", 't', 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', 'hell', \"'\", 's', 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', 'that', \"'\", 's', 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?', 'the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', 'london', \"'\", 's', 'east', 'end', '.', 'it', \"'\", 's', 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.', 'when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.', 'abberline', ',', 'a', 'widower', ',', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', '.', 'upon', 'arriving', 'in', 'whitechapel', ',', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', '(', 'heather', 'graham', ',', 'say', 'it', 'isn', \"'\", 't', 'so', ')', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', 'can', \"'\", 't', 'stomach', '.', 'i', 'don', \"'\", 't', 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', ',', 'so', 'i', 'won', \"'\", 't', 'go', 'into', 'the', 'particulars', 'here', ',', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', '.', 'in', 'the', 'comic', ',', 'they', 'don', \"'\", 't', 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', ',', 'but', 'screenwriters', 'terry', 'hayes', '(', 'vertical', 'limit', ')', 'and', 'rafael', 'yglesias', '(', 'les', 'mis', '?', 'rables', ')', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', '.', 'it', \"'\", 's', 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', ',', 'after', 'all', ',', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', '.', 'and', 'from', 'hell', \"'\", 's', 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', '(', '\"', 'who', 'holds', 'back', 'the', 'electric', 'car', '/', 'who', 'made', 'steve', 'guttenberg', 'a', 'star', '?', '\"', ')', '.', 'don', \"'\", 't', 'worry', '-', 'it', \"'\", 'll', 'all', 'make', 'sense', 'when', 'you', 'see', 'it', '.', 'now', 'onto', 'from', 'hell', \"'\", 's', 'appearance', ':', 'it', \"'\", 's', 'certainly', 'dark', 'and', 'bleak', 'enough', ',', 'and', 'it', \"'\", 's', 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', '(', 'at', 'times', ',', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', ')', '.', 'the', 'print', 'i', 'saw', 'wasn', \"'\", 't', 'completely', 'finished', '(', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', ',', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', ')', ',', 'but', 'cinematographer', 'peter', 'deming', '(', 'don', \"'\", 't', 'say', 'a', 'word', ')', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian', '-', 'era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', ',', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black', '-', 'and', '-', 'white', 'comic', '.', 'oscar', 'winner', 'martin', 'childs', \"'\", '(', 'shakespeare', 'in', 'love', ')', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', '.', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', ',', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', '.', 'ians', 'holm', '(', 'joe', 'gould', \"'\", 's', 'secret', ')', 'and', 'richardson', '(', '102', 'dalmatians', ')', 'log', 'in', 'great', 'supporting', 'roles', ',', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', '.', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', ',', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', ',', 'but', 'it', 'actually', 'wasn', \"'\", 't', 'half', 'bad', '.', 'the', 'film', ',', 'however', ',', 'is', 'all', 'good', '.', '2', ':', '00', '-', 'r', 'for', 'strong', 'violence', '/', 'gore', ',', 'sexuality', ',', 'language', 'and', 'drug', 'content']\n"
          ]
        }
      ],
      "source": [
        "print(list(movie_reviews.words(pos_review_ids[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWztGUZqe5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gUYw61N7iQo"
      },
      "source": [
        "## Creating training and testing sets\n",
        "You will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance.\n",
        "\n",
        "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
        "\n",
        "> Look through the code in the following cell, reading the comments and making sure that you understand each line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HsMcMo5e7iQp"
      },
      "outputs": [],
      "source": [
        "import random # have a look at the documentation at https://docs.python.org/3/library/random.html\n",
        "\n",
        "\n",
        "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
        "    \"\"\"\n",
        "    Given collection of items and ratio:\n",
        "     - partitions the collection into training and testing, where the proportion in training is ratio,\n",
        "\n",
        "    :param data: A list (or generator) of documents or doc ids\n",
        "    :param ratio: The proportion of training documents (default 0.7)\n",
        "    :return: a pair (tuple) of lists where the first element of the\n",
        "            pair is a list of the training data and the second is a list of the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(data)  #Found out number of samples present.  data could be a list or a generator\n",
        "    train_indices = random.sample(range(n), int(n * ratio))          #Randomly select training indices\n",
        "    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n",
        "\n",
        "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
        "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
        "\n",
        "    return (train, test)                       #Return split data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu1pTJhr7iQu"
      },
      "source": [
        "Now we can use this function to create training and testing data.  First, we need to create 4 lists:\n",
        "    * file ids  of positive docs to go in the training data\n",
        "    * file ids of positive docs to go in the testing data\n",
        "    * file ids of negative docs to go in the training data\n",
        "    * file ids of negative docs to go in the testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QUVZGOpJ7iQv"
      },
      "outputs": [],
      "source": [
        "random.seed(41)  #set the random seeds so these random splits are always the same\n",
        "pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
        "neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQjhg51zqe5R"
      },
      "source": [
        "Now, we want to create our labelled data sets.   We need to associate each review with its label so that later we can shuffle up all of the training data (and the testing data)\n",
        "\n",
        "### Exercise 1\n",
        "Write some python code which will construct a training set (`training`) and a test set (`testing`) from the data.  Each set should be a list of pairs where each pair is a list of words and a label, as below:\n",
        "\n",
        "<code>[([list,of,words],'label'),([list,of,words],'label'),...]</code>\n",
        "\n",
        "Hint:  You can do this with 4 list comprehensions and list concatenation.\n",
        "\n",
        "Check the size of `training` and `testing`.  Using a 70\\% split, how many should be in each?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_train = [(movie_reviews.words(sentence), \"training\") for sentence in pos_train_ids]\n",
        "pos_test = [(movie_reviews.words(sentence), \"testing\")  for sentence in pos_test_ids]\n",
        "neg_train = [(movie_reviews.words(sentence), \"training\")  for sentence in neg_train_ids]\n",
        "neg_test = [(movie_reviews.words(sentence), \"testing\")  for sentence in neg_test_ids]\n",
        "\n",
        "train_data = pos_train + neg_train\n",
        "test_data = neg_test + pos_test\n",
        "\n",
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWAKhLcbLkmO",
        "outputId": "38dad375-20ca-4bc9-af57-f0621f18dd6a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['melvin', 'udall', 'is', 'a', 'heartless', 'man', '.', ...], 'training')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yP5wctINNDUS"
      },
      "outputs": [],
      "source": [
        "def labelled_data(pos_data, neg_data):\n",
        "  pos_train_data, pos_test_data = split_data(pos_data)\n",
        "  neg_train_data, neg_test_data = split_data(neg_data)\n",
        "\n",
        "  pos_train = [(movie_reviews.words(sentence), \"training\") for sentence in pos_train_ids]\n",
        "  pos_test = [(movie_reviews.words(sentence), \"testing\")  for sentence in pos_test_ids]\n",
        "  neg_train = [(movie_reviews.words(sentence), \"training\")  for sentence in neg_train_ids]\n",
        "  neg_test = [(movie_reviews.words(sentence), \"testing\")  for sentence in neg_test_ids]\n",
        "\n",
        "\n",
        "\n",
        "  return pos_train, pos_test, neg_train, neg_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kA8AJhWLqe5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eef6ddd-a84c-4b0c-e220-1ab0e2747e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training data: 1400, and it should be 1400\n",
            "Length of test data: 600, and it should be 600\n"
          ]
        }
      ],
      "source": [
        "pos_train, pos_test, neg_train, neg_test = labelled_data(pos_review_ids, neg_review_ids)\n",
        "\n",
        "training = pos_train + neg_train\n",
        "testing = neg_test + pos_test\n",
        "concatenate_data = training + testing\n",
        "print(f\"Length of training data: {len(training)}, and it should be {int(len(concatenate_data)*0.7)}\")\n",
        "print(f\"Length of test data: {len(testing)}, and it should be {int(len(concatenate_data)*0.3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3SLme3Oqe5S"
      },
      "source": [
        "## Document Representations\n",
        "\n",
        "Currently, each review / document is represented as a list of tokens.  In many simple applications, the order of words in a document is deemed irrelevant and we use a bag-of-words representation of the document.  We can create a bag-of-words using a dictionary (as we did in Lab_2_2 when considering the size of the vocabulary) or we can use a library function such as FreqDist from nltk.probability (or Counter from Collections).  In the cell below, I generate the bag-of-words for the first review in the training set using nltk's FreqDist.  You can think of this as like a dictionary but with extra benefits.  For example, later on in the lab, we will see it has useful methods which allow the document representations to be added and subtracted."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ionGa3VgLbH4",
        "outputId": "0e8ca202-78f8-4e18-bae8-984c557fc7a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zRTriMZIqe5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6ff1ff-19af-4479-dc3c-d349ac55867e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "doc1 = FreqDist(training[0][0])\n",
        "doc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4qdRQEeqe5T"
      },
      "source": [
        "### Exercise 2.1\n",
        "\n",
        "Write code to use FreqDist to construct a bag-of-words representation for each document in the training and testing sets.  Store the results in two lists, `training_basic` and `testing_basic`.  Don't lose the annotations as to whether each review is positive or negative!  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_basic = [(FreqDist(words), label) for words, label in training]\n",
        "testing_basic = [(FreqDist(words), label) for words, label in testing]"
      ],
      "metadata": {
        "id": "I2O9MbdXzeho",
        "collapsed": true
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "8Pesd-Rjqe5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce34b7a9-f2fe-4aed-f15d-f8769df38f82",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...}), 'training')\n",
            "(FreqDist({',': 37, '.': 33, 'the': 29, 'a': 22, 'and': 20, \"'\": 16, 'it': 13, 'is': 11, 'to': 11, 'as': 10, ...}), 'testing')\n"
          ]
        }
      ],
      "source": [
        "print(training_basic[0])\n",
        "print(testing_basic[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-sFdi8Jqe5T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJd0x75aqe5U"
      },
      "source": [
        "You will notice of course that many of the words in your representations of documents are punctuation and stopwords.  This is because we haven't done any pre-processing of the wordlists.\n",
        "\n",
        "### Exercise 2.2\n",
        "\n",
        "Decide which of the following pre-processing steps to apply to the word lists:-\n",
        "* case normalisation\n",
        "* number normalisation\n",
        "* punctuation removal\n",
        "* stopword removal\n",
        "* stemmming / lemmatisation\n",
        "\n",
        "\n",
        "Apply these preprocessing steps to the original wordlist representations (stored in `training` and `testing`).  Then recreate the bag-of-words representations, storing the results in `training_norm` and `testing_norm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "DtkW4fzYqe5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30974234-3f23-4f84-f639-b81245a4aed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag = [[word for word in words]for words, label in training]\n",
        "bag = [.lower() for word in bag]"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "gggavPXUQAZT",
        "outputId": "4ec92e8b-0d1b-44e3-9749-4af44d06a2b7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4083206513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def case_normalisation(data):\n",
        "\n",
        "  lower_cased = [token.lower() for token in data]\n",
        "\n",
        "  return lower_cased"
      ],
      "metadata": {
        "id": "WIpqa1xDTjO_"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_lowered = case_normalisation(bag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "UpuA7UUUTz0G",
        "outputId": "376045e9-0eca-4115-8fa5-3c793a721a43"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4199228725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_lowered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase_normalisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2271887687.py\u001b[0m in \u001b[0;36mcase_normalisation\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcase_normalisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mlower_cased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlower_cased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "FLx5ja1aqe5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "844ad364-feaf-4add-8a27-1d4e497b657a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StreamBackedCorpusView' object has no attribute 'isalnum'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-987470416.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtraining_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtraining_stop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-987470416.py\u001b[0m in \u001b[0;36mstopwords_removal\u001b[0;34m(data, language)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StreamBackedCorpusView' object has no attribute 'isalnum'"
          ]
        }
      ],
      "source": [
        "def stopwords_removal(data, language=\"english\"):\n",
        "\n",
        "  stop = stopwords.words(language)\n",
        "  filtered_tokens = [w for w in list(data) if w.isalpha() and w not in stop]\n",
        "  return filtered_tokens\n",
        "\n",
        "\n",
        "training_stop = stopwords_removal(bag)\n",
        "training_stop[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTGHJWSd7iQ5"
      },
      "source": [
        "## Creating word lists\n",
        "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "572x5pEP7iQ6"
      },
      "source": [
        "### Exercise 3.1\n",
        "\n",
        "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
        "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
        "\n",
        "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPzluDd-7iQ6"
      },
      "outputs": [],
      "source": [
        "my_positive_word_list = [\"good\",\"great\",\"lovely\"] # extend this one or put your own list here\n",
        "my_negative_word_list = [\"bad\", \"terrible\", \"awful\"] # extend this one or put your own list here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQhC0Zaqe5W"
      },
      "source": [
        "Now lets see how often each of those words occurs in total in our positive and negative training data.  First, lets create a total of the FreqDists for positive data and for negative data.  As these are FreqDists (rather than simple dictionaries), we can do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tg_0bttqe5X"
      },
      "outputs": [],
      "source": [
        "pos_freq_dist=FreqDist()\n",
        "neg_freq_dist=FreqDist()\n",
        "\n",
        "for reviewDist,label in training_norm:\n",
        "    if label=='pos':\n",
        "        pos_freq_dist+=reviewDist\n",
        "    else:\n",
        "        neg_freq_dist+=reviewDist\n",
        "\n",
        "pos_freq_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNUtrGjVqe5Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW9BMqP37iRH"
      },
      "source": [
        "### Exercise 3.2\n",
        "In the blank code cell below write code that uses the total frequency distributions `pos_freq_dist` and `neg_freq_dist` and the word lists `my_positive_word_list` and `my_negative_word_list` created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
        "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
        "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
        "\n",
        "You could display your findings in a table using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPRJHuET7iRH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4aR84LE7iRL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkfwFU4v7iRO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OefXeFaKqe5b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bXExCJp7iRS"
      },
      "source": [
        "### Exercise 3.3\n",
        "Now, you are going to create positive and negative word lists automatically from the training data. In order to do this:\n",
        "\n",
        "1. write two new functions to help with automating the process of generating wordlists.\n",
        "\n",
        "    - `most_frequent_words` - this function should take THREE arguments: 2 frequency distributions and a natural number, k. It should order words by how much more they occur in one frequency distribution than the other.   It should then return the top k highest scoring words. You might want to use the `most_common` method from the `FreqDist` class - this returns a list of word, frequency pairs ordered by frequency.  You might also or alternatively want to use pythons built-in `sorted` function\n",
        "    - `words_above_threshold` - this function also takes three arguments: 2 frequency distributions and a natural number, k. Again, it should order words by how much more they occur in one distribution than the other.  It should return all of the words that have a score greater than k.\n",
        "\n",
        "2. Using the training data, create two sets of positive and negative word lists using these functions (1 set with each function).\n",
        "3.  Display these 4 lists (possibly in a `Pandas` dataframe?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6hw9SFM7iRY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtwxYJmQ7iRb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP84olqo7iRf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABq5Sb0j_j2p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idr8XYkWAmfl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaqsrbFTAmwI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzVP-hJH7iRi"
      },
      "source": [
        "## Creating a word list based classifier\n",
        "Now you have a number of word lists for use with a classifier.\n",
        "> Make sure you understand the following code, which will be used as the basis for creating a word list based classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSaLWU_A7iRi"
      },
      "outputs": [],
      "source": [
        "from nltk.classify.api import ClassifierI\n",
        "import random\n",
        "\n",
        "class SimpleClassifier(ClassifierI):\n",
        "\n",
        "    def __init__(self, pos, neg):\n",
        "        self._pos = pos\n",
        "        self._neg = neg\n",
        "\n",
        "    def classify(self, words):\n",
        "        score = 0\n",
        "\n",
        "        # add code here that assigns an appropriate value to score\n",
        "        return \"neg\" if score < 0 else \"pos\"\n",
        "\n",
        "    ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
        "    #def classify_many(self, docs):\n",
        "    #    return [self.classify(doc) for doc in docs]\n",
        "\n",
        "    def labels(self):\n",
        "        return (\"pos\", \"neg\")\n",
        "\n",
        "#Example usage:\n",
        "\n",
        "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
        "classifier.classify(FreqDist(\"This movie was great\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irq5PVOc7iRl"
      },
      "source": [
        "### Exercise 3.1\n",
        "\n",
        "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
        "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
        "\n",
        "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`.\n",
        "- For `score` less than 0, \"`neg`\" for negative should be returned.\n",
        "- For `score` greater than 0,  \"`pos`\" for positive should returned.\n",
        "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UXUyFHM7iRm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GRJWPhUF7iRo"
      },
      "source": [
        "### Exercise 3.2\n",
        "* Extend your SimpleClassifier class so that it has a `train` function which will derive the wordlists from training data.  You could build a separate class for each way of automatically deriving wordlists (which both inherit from SimpleClassifier) OR a single class which takes an extra parameter at training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT1PbIao7iRp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h85qqrOV7iRr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uM8En0_7iRu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fWoIb8dDYzR"
      },
      "source": [
        "Try out your classifier on the test data.  We will look at how to evaluate classifiers in the next part, but in an ideal world, most of the positive test items will have been classified as 'P' and most of the negative test items will have been classified as 'N'.  Note that the batch_classify method takes a list of unlabelled documents so you can't give it a list of pairs (where each pair is doc and a label).  You can either use a list comprehension or the <code>zip(*list_of_pairs)</code> function to split a list of pairs into a pair of lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6fQs92SVu39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFCEF0rx7iRx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us0zxFpsqe5g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9MTBKOuDPU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuS6Urkaqe5g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nphj7NkSqe5g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}