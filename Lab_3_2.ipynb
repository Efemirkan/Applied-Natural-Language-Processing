{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjIgwH4mURxe"
      },
      "source": [
        "# Lab 3: Document Classification (Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCv5KJOCURxf"
      },
      "source": [
        "### Overview\n",
        "This topic builds on the activities of the previous topic on sentiment analysis. You will be focussing on the movie review corpus with a view to investigating the following issues.\n",
        "\n",
        "- Evaluation metrics for classifier performance\n",
        "- What is the impact of varying training data size? To what extent does increasing the quantity of training data improve classifier performance?\n",
        "\n",
        "By this stage, you should be very comfortable with Python's [list comprehensions](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) and [slice](http://bergbom.blogspot.co.uk/2011/04/python-slice-notation.html) notation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LirziHXVURxh"
      },
      "source": [
        ">To access functionality defined in previous notebooks, copy the functions defined in Week3Labs into a `utils.py` file and then import it into the notebook.  There is a `utils.py` file included with these resources which you can update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Im4wuOJr66N",
        "outputId": "0d0c93e6-9d02-45d0-b1ac-ea67770cc152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "##uncomment the following lines on colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtr8YFODsUUh",
        "outputId": "ad23e597-2ad2-4e6b-9d9c-bd78cf4dcc23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#other needed imports\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "import pandas as pd\n",
        "#this next line will ensure pandas graphs are plotted in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "##download stopwords and movie_reviews - need to do this every session on colab\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb62QpRpURxi"
      },
      "outputs": [],
      "source": [
        "#set your system path so that jupyter knows where to look for other files\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/NLE Notebooks 2021/Week3LabsSolutions/')\n",
        "#import code to setup training and testing data, wordlist classifiers and NB classifiers\n",
        "from utils import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random # have a look at the documentation at https://docs.python.org/3/library/random.html\n",
        "\n",
        "\n",
        "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
        "    \"\"\"\n",
        "    Given collection of items and ratio:\n",
        "     - partitions the collection into training and testing, where the proportion in training is ratio,\n",
        "\n",
        "    :param data: A list (or generator) of documents or doc ids\n",
        "    :param ratio: The proportion of training documents (default 0.7)\n",
        "    :return: a pair (tuple) of lists where the first element of the\n",
        "            pair is a list of the training data and the second is a list of the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(data)  #Found out number of samples present.  data could be a list or a generator\n",
        "    train_indices = random.sample(range(n), int(n * ratio))          #Randomly select training indices\n",
        "    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n",
        "\n",
        "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
        "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
        "\n",
        "    return (train, test)                       #Return split data"
      ],
      "metadata": {
        "id": "3cFvRgVHzb7B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "Pn6uw0rd0CP1",
        "outputId": "32693103-970f-4b45-984e-218d0149bed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_tokens(data):\n",
        "\n",
        "  # C normalisation\n",
        "  case_normalised_token = [token.lower() for token in data]\n",
        "\n",
        "  # Replace digits with \"NUM\"\n",
        "  normalised_token_first = [\"NUM\" if token.isdigit() else token for token in case_normalised_token]\n",
        "\n",
        "  # Replace ordinals with \"Nth\"\n",
        "  normalised_token = [\"Nth\" if (token.endswith((\"nd\",\"st\",\"th\",\"rd\")) and token[:-2].isdigit()) else token for token in normalised_token_first]\n",
        "\n",
        "  return normalised_token\n",
        "\n",
        "def stopword_removal(data):\n",
        "  stop = stopwords.words('english')\n",
        "\n",
        "  # Remove stopwords and token not made of only letters\n",
        "  filtered_token_first = [token for token in data if token.isalpha() and token not in stop ]\n",
        "\n",
        "  # Remove \"NUM\" and \"Nth\"\n",
        "  filtered_token = [token for token in filtered_token_first if token not in [\"NUM\", \"Nth\"]]\n",
        "\n",
        "  return filtered_token\n",
        "\n",
        "def lemmatize_tokens(data):\n",
        "  lem = WordNetLemmatizer()\n",
        "\n",
        "  # Lemmatize the tokens\n",
        "  lemmatize_token = [lem.lemmatize(token) for token in data]\n",
        "\n",
        "  return lemmatize_token\n",
        "\n",
        "# Perform full preprocessing\n",
        "def normalise(data):\n",
        "  normalised_reviews = normalized_tokens(data)\n",
        "  filtered_reviews = stopword_removal(normalised_reviews)\n",
        "  cleaned_data = lemmatize_tokens(filtered_reviews)\n",
        "\n",
        "  return cleaned_data"
      ],
      "metadata": {
        "id": "ddxNVWvRzuOR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Sqxr7yhw0m0I",
        "outputId": "619977e6-ec3d-4a57-dd9a-4e72cb537ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "Jh9hLEJRzz4w"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mZap-2rURxr"
      },
      "source": [
        "## Evaluation Metrics for Classifier Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juCwCS-dURxr"
      },
      "source": [
        "### Accuracy\n",
        "Here is code for an evaluation function <code>evaluate_wordlist_classifier</code> which can be used to determine how well a word_list classifier performs. This function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified.  Look at the code and make sure you understand what it is doing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Kf4kWgpZCLYi"
      },
      "outputs": [],
      "source": [
        "def classifier_evaluate(cls, test_data):\n",
        "    '''\n",
        "    cls: an instance of a classifier object which has a classify method which returns \"pos\" or \"neg\"\n",
        "    test_data: a list of pairs where each pair is a FreqDist rep of a doc and its label\n",
        "\n",
        "    returns: float point number which is the accuracy of the classifier on the test data provided\n",
        "    '''\n",
        "    acc = 0\n",
        "    docs,goldstandard=zip(*test_data) #note this neat pythonic way of turning a list of pairs into a pair of lists\n",
        "    #pass all of the docs to the classifier and get back a list of predictions\n",
        "    predictions=cls.classify_many(docs)\n",
        "    #zip the predictions with the goldstandard labels and compare\n",
        "    for prediction,goldlabel in zip(predictions,goldstandard):\n",
        "        if prediction==goldlabel:\n",
        "            acc+=1\n",
        "\n",
        "    return acc / (len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki1vS7WGVYQ4"
      },
      "source": [
        "Now we need some data to train and test our classifier on.  We are going to make use of the code from the lab_3_1.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aKjfbLU0VYd9"
      },
      "outputs": [],
      "source": [
        "def get_train_test_data():\n",
        "\n",
        "    #get ids of positive and negative movie reviews\n",
        "    pos_review_ids=movie_reviews.fileids('pos')\n",
        "    neg_review_ids=movie_reviews.fileids('neg')\n",
        "\n",
        "    #split positive and negative data into training and testing sets\n",
        "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
        "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
        "    #add labels to the data and concatenate\n",
        "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
        "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
        "    #now normalise and create bag-of-words FreqDist representations\n",
        "    training_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in training]\n",
        "    testing_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in testing]\n",
        "    return training_norm, testing_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NsOsDS2aJXY",
        "outputId": "364f5023-b8c8-45f0-aa31-b82240ce908a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(FreqDist({'melvin': 6, 'simon': 4, 'dog': 4, 'day': 3, 'playing': 3, 'one': 2, 'serve': 2, 'carol': 2, 'threatens': 2, 'shut': 2, ...}),\n",
              " 'pos')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "random.seed(41)  #set the random seeds so these random splits are always the same\n",
        "training,testing=get_train_test_data()\n",
        "\n",
        "training[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9kB9w0Dv_k"
      },
      "source": [
        "We are now going to use this function to start evaluating our classifiers from last week.  Lets first try the SimpleClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_words(freq1,freq2,k):\n",
        "\n",
        "    difference = freq1-freq2  # frequency differences\n",
        "    sorted_diff = difference.most_common()  # sorted by highest difference\n",
        "    mostwords = [token for (token, freq) in sorted_diff[:k]]  # collect top k tokens\n",
        "\n",
        "    return mostwords"
      ],
      "metadata": {
        "id": "LDsBRBQl1BRM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify.api import ClassifierI\n",
        "\n",
        "class SimpleClassifier(ClassifierI):\n",
        "\n",
        "    def __init__(self, pos, neg):\n",
        "        self._pos = pos\n",
        "        self._neg = neg\n",
        "\n",
        "    def classify(self, words):\n",
        "        score = 0\n",
        "\n",
        "        # add code here that assigns an appropriate value to score\n",
        "        return \"neg\" if score < 0 else \"pos\"\n",
        "\n",
        "    def labels(self):\n",
        "        return (\"pos\", \"neg\")"
      ],
      "metadata": {
        "id": "ljytzX-20za9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9b7hwEuURxs",
        "outputId": "f48d75b0-baa0-4c72-8000-af7b4a03bad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "#here I am going to create an instance of a very simple classifier\n",
        "\n",
        "my_positive_word_list = [\"good\",\"great\",\"lovely\", \"excellent\"] # extend this one or put your own list here\n",
        "my_negative_word_list = [\"bad\", \"terrible\", \"awful\", \"dreadful\"] # extend this one or put your own list here\n",
        "movie_classifier1 = SimpleClassifier(my_positive_word_list,my_negative_word_list)\n",
        "\n",
        "#Evaluate classifier\n",
        "#The function requires two arguments:\n",
        "# 1. Word list based classifer\n",
        "# 2. A list (or generator) of labelled test items\n",
        "score = classifier_evaluate(movie_classifier1, testing)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCS5zwlURxv"
      },
      "source": [
        "If you have run the cell above without updating the SimpleClassifier code you should see that the accuracy is 0.5 i.e., 50%. The original SimpleClassifier just assigns everything to the positive class.  Since it is a binary classification decision and the classes are balanced, it will get 50% of the decisions correct (those that are positive) and 50% of the decisions incorrect (those that are actually negative).  This is the **baseline** result for this kind of classification task.  We obviously want to build classifiers that do better than this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxDIMt53EUby"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Now you try one of the classifiers that you wrote that selects positive and negative words from the training data.  Hopefully, this will perform better than the baseline of 50%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlNs8VBdET2e"
      },
      "outputs": [],
      "source": [
        "#Create a new classifier\n",
        "#Make sure you have updated the code in utils.py to contain your WordList Classifier\n",
        "#If you update the utils.py code mid-session, you will need to restart the runtime / kernel in order to force it to import the new updated code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QKxbaLFtURxw"
      },
      "outputs": [],
      "source": [
        "def most_frequent_words(freq1,freq2,k):\n",
        "\n",
        "    difference = freq1-freq2  # frequency differences\n",
        "    sorted_diff = difference.most_common()  # sorted by highest difference\n",
        "    mostwords = [token for (token, freq) in sorted_diff[:k]]  # collect top k tokens\n",
        "\n",
        "    return mostwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(ClassifierI):\n",
        "\n",
        "    def __init__(self, k):\n",
        "        # Store k\n",
        "        self._k = k\n",
        "\n",
        "    def classify(self, doc):\n",
        "        # Compute how many positive and negative tokens appear\n",
        "        score = 0\n",
        "\n",
        "        for word, count in doc.items():\n",
        "          if word in self._pos:\n",
        "            score += count  # Increase score for positive tokens\n",
        "          if word in self._neg:\n",
        "            score-= count  # Decrease score for negative tokens\n",
        "\n",
        "        # Decide final label based on score\n",
        "        if score > 0:\n",
        "          return \"pos\"\n",
        "        elif score == 0:\n",
        "          return random.choice([\"pos\", \"neg\"])  # Resolve tied cases\n",
        "        else:\n",
        "          return \"neg\"\n",
        "\n",
        "    def train(self,training_data):\n",
        "\n",
        "        # Build frequency distributions for positive and negative reviews\n",
        "        pos_freq_dist=FreqDist()\n",
        "        neg_freq_dist=FreqDist()\n",
        "\n",
        "        for reviewDist,label in training_data:\n",
        "            if label=='pos':\n",
        "                pos_freq_dist+=reviewDist\n",
        "            else:\n",
        "                neg_freq_dist+=reviewDist\n",
        "\n",
        "        # Get the k most frequent tokens\n",
        "        self._pos=most_frequent_words(pos_freq_dist,neg_freq_dist,self._k)\n",
        "        self._neg=most_frequent_words(neg_freq_dist,pos_freq_dist,self._k)\n",
        "\n",
        "    def labels(self):\n",
        "        # Return the class labels\n",
        "        return (\"pos\", \"neg\")"
      ],
      "metadata": {
        "id": "p8d3nV-21wz_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_classifier2 = SimpleClassifier(100)\n",
        "movie_classifier2.train(training)"
      ],
      "metadata": {
        "id": "remhOUrk2F6H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = classifier_evaluate(movie_classifier2, testing)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "W908Ep715hfl",
        "outputId": "d9f210c5-17e7-4823-e9c7-44b6c759bddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6483333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm7UGczgURyd"
      },
      "source": [
        "## Precision, Recall and F1 score etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74BIeN6URye"
      },
      "source": [
        "When classes are unbalanced, evaluating classifiers in terms of accuracy can be misleading.  For example, if 10% of documents are relevant and 90% of documents are irrelevant, then a classifier which labels all documents as irrelevant will obtain an accuracy of 90%.  This sounds good but is actually useless. More useful metrics for evaluation of performance are precision, recall and F1 score.  These metrics allow us to distinguish the different types of errors our classifiers make.\n",
        "\n",
        "For each class, $c$, we need to keep a record of\n",
        "* True Positives: $TP=|\\{i|\\mbox{prediction}(i)=\\mbox{label}(i)=c\\}|$\n",
        "* False Negatives: $FN=|\\{i|\\mbox{prediction}(i)\\neq \\mbox{label}(i)=c\\}|$\n",
        "* False Positives: $FP=|\\{i|\\mbox{label}(i) \\neq \\mbox{prediction}(i)=c\\}|$\n",
        "* True Negatives: $TN=|\\{i|\\mbox{prediction}(i)=\\mbox{label}(i)\\neq c\\}|$\n",
        "\n",
        "Note the symmetry in the binary classification task (the TN for one class are the TP for the other class and so on).  Therefore, in binary classification, we just record these values and compute the following evaluation metrics for a single class (e.g. \"Relevant\" or \"Positive\")\n",
        "\n",
        "* Precision:\n",
        "\\begin{eqnarray*}\n",
        "P=\\frac{TP}{TP+FP}\n",
        "\\end{eqnarray*}\n",
        "* Recall:\n",
        "\\begin{eqnarray*}\n",
        "R=\\frac{TP}{TP+FN}\n",
        "\\end{eqnarray*}\n",
        "* F1-score:\n",
        "\\begin{eqnarray*}\n",
        "F1 = \\frac{2\\times P\\times R}{P+R}\n",
        "\\end{eqnarray*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvEQsGi0URye"
      },
      "source": [
        " ### Exercise 2.1\n",
        "\n",
        " The code below defines a ConfusionMatrix class for the binary classification task.  Currently, it will compute the number of TPs, FPs, FNs and TNs.  Test it out with predictions and test data for our sentiment analysis task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhIJN-9URyf"
      },
      "outputs": [],
      "source": [
        "class ConfusionMatrix:\n",
        "    def __init__(self,predictions,goldstandard,classes=(\"pos\",\"neg\")):\n",
        "\n",
        "        (self.c1,self.c2)=classes\n",
        "        self.TP=0\n",
        "        self.FP=0\n",
        "        self.FN=0\n",
        "        self.TN=0\n",
        "        for p,g in zip(predictions,goldstandard):\n",
        "            if g==self.c1:\n",
        "                if p==self.c1:\n",
        "                    self.TP+=1\n",
        "                else:\n",
        "                    self.FN+=1\n",
        "\n",
        "            elif p==self.c1:\n",
        "                self.FP+=1\n",
        "            else:\n",
        "                self.TN+=1\n",
        "\n",
        "\n",
        "    def precision(self):\n",
        "        p=0\n",
        "        #put your code to compute precision here\n",
        "\n",
        "        return p\n",
        "\n",
        "    def recall(self):\n",
        "        r=0\n",
        "        #put your code to compute recall here\n",
        "\n",
        "        return r\n",
        "\n",
        "    def f1(self):\n",
        "        f1=0\n",
        "        #put your code to compute f1 here\n",
        "\n",
        "        return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUOy77CNURyk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNsc3JO5URyx"
      },
      "source": [
        "### Exercise 2.2\n",
        "* Add functionality to the ConfusionMatrix class code to compute precision, recall and F1 score\n",
        "* Use your code to evaluate the performance of the different classifiers you have constructed.\n",
        "* Interpret your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKKAh-8iURyx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j38Nt0rPURy1"
      },
      "source": [
        "## Investigating the impact of the quantity of training data\n",
        "We will begin by exploring the impact on classification accuracy of using different quantities of training data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nw5k_3WURy3"
      },
      "source": [
        "Run the code in the cell below several times.  Each time it should generate a new sample of review data, train the classifiers and evaluate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTu-mhEkURy3"
      },
      "outputs": [],
      "source": [
        "\n",
        "training,testing=get_train_test_data()\n",
        "\n",
        "word_list_size = 100\n",
        "threshold=100\n",
        "classifiers={\"Word List MF\":SimpleClassifier_mf(word_list_size),\n",
        "             \"Word List Thresh\":SimpleClassifier_ot(threshold)}\n",
        "\n",
        "results=[]\n",
        "for name,classifier in classifiers.items():\n",
        "    classifier.train(training)\n",
        "    accuracy=classifier_evaluate(classifier,testing)\n",
        "    print(\"The accuracy of {} classifier is {}\".format(name,accuracy))\n",
        "    results.append((name,accuracy))\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "display(df)\n",
        "ax = df.plot.bar(title=\"Experimental Results\",legend=False,x=0)\n",
        "ax.set_ylabel(\"Classifier Accuracy\")\n",
        "ax.set_xlabel(\"Classifier\")\n",
        "ax.set_ylim(0,1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C08xFmkcURy6"
      },
      "source": [
        "As you can see, the classifiers have different accuracies on different runs.\n",
        "\n",
        "### Exercise 3.1\n",
        "Copy the cell above and move the copy to be positioned below this cell. Then adapt the code so that the accuracy reported for each classifier is the average across multiple runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMxDKCTYrzxT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_vzc-cURzJ"
      },
      "source": [
        "### Exercise 3.2\n",
        "Adapt the code so that it calculates average precision, recall and F1-score rather than average accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu99TOHErzxU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP50O9lgURzM"
      },
      "source": [
        "### Exercise 3.3\n",
        "Investigate the impact of training data size on the performance of both word list classifiers, using a range of subsets of movie reviews.\n",
        "\n",
        "Hints and tips.\n",
        "- You can copy the code cell that you created for the last exercise, and place the copy below this cell. Then adapt the code to determine accuracy, precision, recall and F1-score for each classifier on each subset.\n",
        "\n",
        "- Use the `sample` function from the random module.\n",
        "- Remember, the full data set has 1000 positive and 1000 negative reviews.\n",
        "- You should continue to use 30% of the data for testing, so this means that we have up to 700 positive and 700 negative reviews to sample from.\n",
        "- Make sure that you are selecting samples that have an equal number of positive and negative reviews.\n",
        "- Consider (at least) the following sample sizes: 2, 10, 50, 100, 200, 400, 600 and 700.\n",
        "- Note that the sample size is not the total number of reviews, but the number of positive reviews (which is also equal to the number of negative reviews).\n",
        "\n",
        "\n",
        "**Interpret your results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDNqN7QFURzN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfg4Y9UgrzxV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr7t6a5El82f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA-qFH0vuGse"
      },
      "source": [
        "My answer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRi4wCz3mElF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d9oBaz2URzR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab_3_2_SOLUTONS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}